#!/bin/sh
# config file for system

# the project id for name prefix
HDFF_PROJ_ID=mr4ns2
# description
HDFF_PROJ_DESC="Map-Reduce for NS2 simulations"

# how many running processes in each node
# 0 -- auto detect the CPU cores, use about 5/7 of them
HDFF_NUM_CLONE=0

# total number of nodes (machines) in the system, default = 1
HDFF_TOTAL_NODES=1

#EXEC_NS2="$(which ns)"
HDFF_FN_LOG="/dev/null"
#HDFF_FN_LOG="mylog.txt"

# the user start the task
# please set it in your start script or by manual
HDFF_USER=${USER}

# we have to use /tmp file for both local and hdfs file systems,
# since the runner may not be the user submitted the job;
# and the /tmp(or /dev/shm) is the only directory that can be
# accessed by both the user and runner (and also other users)

HDFF_DN_BASE="hdfs:///tmp/${HDFF_USER}"

# the output file directory
#HDFF_DN_OUTPUT=mapreduce-results
#HDFF_DN_OUTPUT=hdfs://${HDFF_DN_BASE}/mapreduce-results/
#HDFF_DN_OUTPUT="hdfs:///user/${USER}/mapreduce-results/"
#HDFF_DN_OUTPUT="file://$HOME/mapreduce-results/"
#HDFF_DN_OUTPUT="file:///scratch1/$USER/mapreduce-results/"
HDFF_DN_OUTPUT=${HDFF_DN_BASE}/mapreduce-results/

# the temporary directory for NS2 simulator
#HDFF_DN_SCRATCH="${HDFF_DN_BASE}/"
#HDFF_DN_SCRATCH="/run/shm/${HDFF_USER}/"
#HDFF_DN_SCRATCH="/dev/shm/${HDFF_USER}/"
#HDFF_DN_SCRATCH="/local_scratch/${HDFF_USER}/"
HDFF_DN_SCRATCH=/dev/shm/${HDFF_USER}/

# the directory for save the un-tar binary files
# it should be a directory in a local disk
#HDFF_DN_BIN="/run/shm/${HDFF_USER}/bin"
#HDFF_DN_BIN="/dev/shm/${HDFF_USER}/bin"
HDFF_DN_BIN=/dev/shm/${HDFF_USER}/bin

# the tar file for application binary
#HDFF_FN_TAR_APP=hdfs://${HDFF_DN_BASE}/ns2docsis-ds31profile-i386-compiled.tar.gz
HDFF_FN_TAR_APP=

# the tar file for mrnative-ns2
#HDFF_FN_TAR_MRNATIVE=hdfs://${HDFF_DN_BASE}/mrnativens2-1.1.tar.gz
HDFF_FN_TAR_MRNATIVE=

